# -*- coding: utf-8 -*-
"""practice_cancer_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19umaCJqlVlged5Gx6knQtZeCtQgCIIcj
"""

!apt-get install texlive-xetex texlive-fonts-recommended texlive-full -y

from nbconvert import PDFExporter
import nbformat

# Load the notebook
notebook_filename = '/content/gdrive/My Drive/Colab Notebooks/practice_cancer_detection.ipynb'  # Replace with your notebook's filename
with open(notebook_filename, 'r', encoding='utf-8') as notebook_file:
    notebook_content = nbformat.read(notebook_file, as_version=4)

# Create a PDFExporter instance
pdf_exporter = PDFExporter()

# Export to PDF with outputs
(body, resources) = pdf_exporter.from_notebook_node(notebook_content)

# Save the PDF to a file
pdf_filename = 'cancer_detection_notebook.pdf'  # Replace with your desired output PDF filename
with open(pdf_filename, 'wb') as pdf_file:
    pdf_file.write(body)

from google.colab import files

# Download the PDF file
files.download('cancer_detection_notebook.pdf')  # Replace with your PDF filename

"""<h1 align= center>Cancer Prediction Case Study"""

from google.colab import drive
drive.mount("/content/gdrive")

import pandas as pd
import numpy as np
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
import warnings
warnings.filterwarnings('ignore')
import re
import time
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix , log_loss
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.calibration import CalibratedClassifierCV
import math
from collections import Counter , defaultdict
from sklearn.preprocessing import normalize
from scipy.sparse import hstack
from scipy.sparse import csr_matrix
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
import sys   # for stacking
import six   # for stacking
sys.modules['sklearn.externals.six'] = six
from mlxtend.classifier import StackingClassifier

"""<h3>Importing Gene, Variation & Class data


"""

df_gene_var = pd.read_csv("/content/gdrive/My Drive/Cancer Prediction/training_variants")
print("Number of data points: ",df_gene_var.shape[0])
print("Number of Features: ",df_gene_var.shape[1])
print("Features: ", df_gene_var.columns.values)
df_gene_var.head()

"""<h3>Importing Text data"""

df_text = pd.read_csv("/content/gdrive/MyDrive/Cancer Prediction/training_text", sep='\|\|', engine='python', names=['ID', 'TEXT'] , skiprows=1)
print("Number of data points: ",df_text.shape[0])
print("Number of Features: ",df_text.shape[1])
print("Features: ",df_text.columns.values)
df_text.head()

"""<h3>Some little preprocessing"""

# importing stopwords
stop_words = set(stopwords.words('english'))
# creating function for preprocessing text
def nlp_preprocessing(total_text, index, column):
  if type(total_text) is not int:
    string = ""
    # replace every special chracter with splace
    total_text = re.sub('[^a-zA-Z0-9\n]',' ',total_text)
    # replace multiple spaces with single space
    total_text = re.sub('\s+',' ', total_text)
    # lower case the text
    total_text = total_text.lower()

    # Take the sentence and change the o/p to words
    # split() function working---> I/P "welcome to the jungle"    O/P['welcome', 'to', 'the', 'jungle']
    for word in total_text.split():
      if word not in stop_words:
        string += word + ' '
    df_text[column][index] = string

#
start_time = time.clock()
for index, row in df_text.iterrows():
  if type(row['TEXT']) is str:
    nlp_preprocessing(row['TEXT'],index,'TEXT')
  else:
    print("There is no Description for Index: ",index)
print("Total Time Taken for nlp preprocessing: ",time.clock() - start_time," seconds")

"""<h3>Merging both gene, Variation and Text dataset"""

result = pd.merge(df_gene_var, df_text, on='ID', how='left')
print("Shape of resultant data: ",result.shape)
result.head()

#Finding the null values
result[result.isnull().any(axis=1)]

# Now adding gene and variation inplace of null text
result.loc[result['TEXT'].isnull(),'TEXT'] = result['Gene'] + " " + result['Variation']
result[result['ID'] == 1109]
# result['ID'] == 1109  df inside df
# 0       False
# 1       False
# 2       False
# 3       False
# 4       False

#       ...
# 3316    False
# 3317    False
# 3318    False
# 3319    False
# 3320    False
# Name: ID, Length: 3321, dtype: bool

"""<h3> Splitting of data for Train, Cross Validate and Test."""

y = result['Class']
#result = result.drop('Class',axis=1)
# 80% Train data and 20% Test data
x_train1, x_test, y_train1, y_test = train_test_split(result , y , stratify= y , test_size= 0.2)
# 80% Train data and 20% Test data
x_train, x_cv, y_train, y_cv = train_test_split(x_train1, y_train1, stratify= y_train1, test_size=0.2)

x_train.head(2)

print("Number of data point in train set: ",x_train.shape[0])
print("Number of data point in Cross Validate set: ",x_cv.shape[0])
print("Number of data point in Test set: ",x_test.shape[0])

"""<h3> Now it's time to analyse the Distribution of data."""

train_class_distribution = x_train['Class'].value_counts().sort_index()
cv_class_distribution    = x_cv['Class'].value_counts().sort_index()
test_class_distribution  = x_test['Class'].value_counts().sort_index()


train_class_distribution.plot(kind='bar')
plt.xlabel('Classes')
plt.ylabel('Frequency of data points')
plt.title("Distribution of Yi's in Train dataset")
plt.grid()
plt.show()

sorted_yi = np.argsort(-train_class_distribution.values)
for i in sorted_yi:
  print("Number of data points in class ",i+1,":",train_class_distribution.values[i],"(",np.round(train_class_distribution.values[i]/y_train.shape[0]*100,3),"%)")


print("-"*150)
cv_class_distribution.plot(kind='bar')
plt.xlabel("Classes")
plt.ylabel("Frequency of data points")
plt.title("Distribution of Yi's in CV dataset")
plt.grid()
plt.show()

sorted_y = np.argsort(-cv_class_distribution.values)
for i in sorted_y:
  print("Number of data points in class ",i+1,":",cv_class_distribution.values[i],"(",np.round(cv_class_distribution.values[i]/y_cv.shape[0]*100,3),"%)")


print("-"*150)
cv_class_distribution.plot(kind='bar')
plt.xlabel("Classes")
plt.ylabel("Frequency of data points")
plt.title("Distribution of Yi's in Test dataset")
plt.grid()
plt.show()

sorted_y = np.argsort(-cv_class_distribution.values)
for i in sorted_y:
  print("Number of data points in class ",i+1,":",test_class_distribution.values[i],"(",np.round(test_class_distribution.values[i]/y_test.shape[0]*100,3),"%)")

"""By observing Gene feature we observe that their train, cv, and test dataset follows similar distribution which is good."""

def plot_confusion_matrix(y_test , predicted_y):
  C = confusion_matrix(y_test , predicted_y)
  A = (((C.T)/(C.sum(axis=1))).T)   # precison column sum
  B = (C/C.sum(axis=0))       # recall   row sum

  label = [1,2,3,4,5,6,7,8,9]
  plt.figure(figsize=(20,7))
  sns.heatmap(C, annot=True, cmap="YlGnBu", fmt="0.3f", xticklabels=label, yticklabels=label)
  print("-"*60, "Confusion matrix", "-"*60)
  plt.xlabel("Predicted Class")
  plt.ylabel("Original Class")
  plt.show()

  plt.figure(figsize=(20,7))
  sns.heatmap(A, fmt='0.3f', annot=True, cmap="YlGnBu" , xticklabels=label , yticklabels=label)
  print("-"*60,"Precision Martix axis=1","-"*60)
  plt.xlabel("Predicted Class")
  plt.ylabel("Original Class")
  plt.show()

  plt.figure(figsize=(20,7))
  sns.heatmap(B, fmt='0.3f', annot=True, cmap="YlGnBu" , xticklabels=label , yticklabels=label)
  print("-"*60,"Recall Martix axis=0","-"*60)
  plt.xlabel("Predicted Class")
  plt.ylabel("Original Class")
  plt.show()

train_data_len = x_train.shape[0]  # length is 2124
cv_data_len    = x_cv.shape[0]     # length is 532
test_data_len  = x_test.shape[0]   # length is 665
# we need to generate random 9 random numbers and their sum should be 1.
# One solution is to generate 9 numbers and divide each number with their sum
train_predict_y = np.zeros((train_data_len,9))
for i in range(train_data_len):
  rand_probs = np.random.rand(1,9)
  train_predict_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])
print("logloss for train dataset is: ", log_loss(y_train , train_predict_y, eps=1e-15))

cv_predict_y = np.zeros((cv_data_len,9))
for i in range(cv_data_len):
  rand_probs = np.random.rand(1,9)
  cv_predict_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])
print("Logloss for Cross Validate dataset is: ",log_loss(y_cv , cv_predict_y, eps=1e-15))

test_predict_y = np.zeros((test_data_len,9))
for i in range(test_data_len):
  rand_probs = np.random.rand(1,9)
  test_predict_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])
print("Logloss for test dataset is: ",log_loss(y_test , test_predict_y , eps=1e-15))
predicted_y = np.argmax(test_predict_y,axis=1)
plot_confusion_matrix(y_test , predicted_y+1)

# Function for response coding
def get_gv_feat_dict(alpha, feature,df):
  value_count = x_train[feature].value_counts()
  gv_dict=dict()
  for i, denominator in value_count.items():
    vec = []
    for k in range(1,10):
      cls_cnt = x_train.loc[(x_train['Class']==k) & (x_train[feature]==i)]
      vec.append((cls_cnt.shape[0] + alpha*10) / (denominator + alpha*90))
    gv_dict[i] = vec
  return gv_dict

def get_gv_feature(alpha, feature, df):
  gv_dict     = get_gv_feat_dict(alpha, feature, df)
  value_count = x_train[feature].value_counts()
  gv_feat=[]
  for index,row in df.iterrows():
    if row[feature] in dict(value_count).keys():
      gv_feat.append(gv_dict[row[feature]])
    else:
      gv_feat.append([1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9, 1/9])
  return gv_feat

"""<h3>Univarient Analysis of Gene Feature"""

unique_gene = x_train['Gene'].value_counts()
print("Number of unique genes: ",unique_gene.shape[0])
print("Top 10 Gene features in train dataset: ")
top_10_g = x_train['Gene'].value_counts()
top_10_g.head(10)

# plotting histogram for Gene feature
s = sum(unique_gene.values)
h = unique_gene.values/s
plt.plot(h,label="Histogram for Gene feature")
plt.xlabel("Number of Gene features")
plt.ylabel("Number of Occurance")
plt.grid()
plt.legend()
plt.show()

c = np.cumsum(h)
plt.plot(c, label="CDF of Gene feature")
plt.legend()
plt.xlabel("Number of Genes feature")
plt.ylabel("Occurance")
plt.grid()
plt.show()

alpha = 1
train_gene_ResponseCoding = np.array(get_gv_feature(alpha,'Gene', x_train))
cv_gene_ResponseCoding    = np.array(get_gv_feature(alpha,'Gene', x_cv))
test_gene_ResponseCoding  = np.array(get_gv_feature(alpha, 'Gene', x_test))
print("train_gene_feature_responseCoding is converted feature using respone coding method. The shape of gene feature:", train_gene_ResponseCoding.shape)
print("cv_gene_feature_responseCoding is converted feature using respone coding method. The shape of gene feature:", cv_gene_ResponseCoding.shape)
print("testn_gene_feature_responseCoding is converted feature using respone coding method. The shape of gene feature:", test_gene_ResponseCoding.shape)

gene_encoding = CountVectorizer()
train_gene_onehotencode  = gene_encoding.fit_transform(x_train['Gene'])  # One time fit
cv_gene_onehotencode     = gene_encoding.transform(x_cv['Gene'])    # No need to fit
test_gene_onehotencode = gene_encoding.transform(x_test['Gene'])
print("train_gene_onehotencoding is converted feature using onehot encoding method. The shape of gene feature:", train_gene_onehotencode.shape)
print("cv_gene_onehotencoding is converted feature using onehot encoding method. The shape of gene feature:", cv_gene_onehotencode.shape)
print("test_gene_onehotencoding is converted feature using onehot encoding method. The shape of gene feature:", test_gene_onehotencode.shape)

gene_encoding.get_feature_names()

"""<h3>Modelling using Logistic Regression"""

cv_log_error = []
alpha = [10 ** x for x in range(-5 , 2 )]
for i in alpha:
  clf   = SGDClassifier(alpha= i, penalty= 'l2' , loss='log', random_state= 42, n_jobs= -1)
  clf.fit(train_gene_onehotencode, y_train)
  sig_clf = CalibratedClassifierCV(clf, method='sigmoid')
  sig_clf.fit(train_gene_onehotencode, y_train)
  y_predicted = sig_clf.predict_proba(cv_gene_onehotencode)
  cv_log_error.append(log_loss(y_cv, y_predicted, labels= clf.classes_ , eps=1e-15))
  print("LogLoss for alpha",i,"is: ",log_loss(y_cv, y_predicted, labels= clf.classes_, eps=1e-15))

fig,ax = plt.subplots()
ax.plot(alpha, cv_log_error, c='g')
for i, txt in enumerate(np.round(cv_log_error,3)):
  ax.annotate((alpha[i],np.round(txt,3)), (alpha[i], cv_log_error[i]))
plt.title("Cross Validate error for each aplha")
plt.xlabel("Alphas'i")
plt.ylabel("Log error")
plt.grid()
plt.show()

best_alpha= np.argmin(cv_log_error)
clf = SGDClassifier(alpha = alpha[best_alpha], penalty='l2', random_state=42, loss='log', n_jobs=-1)
clf.fit(train_gene_onehotencode, y_train)
sig_clf = CalibratedClassifierCV(clf, method = 'sigmoid')
# sig_clf = CalibratedClassifierCV(clf, method = 'isotonic')
# For value of best alpha 1  the logloss is:  0.9549938260739086
# For value of best alpha 1  the logloss is:  1.3962269548727062
# For value of best alpha 1  the logloss is:  1.3576217932980164

sig_clf.fit(train_gene_onehotencode, y_train)

y_predicted = sig_clf.predict_proba(train_gene_onehotencode)
print("For value of best alpha", alpha[best_alpha] , " the logloss is: ",log_loss(y_train, y_predicted, labels=clf.classes_, eps=1e-15))

y_predicted = sig_clf.predict_proba(cv_gene_onehotencode)
print("For value of best alpha", alpha[best_alpha], " the logloss is: ", log_loss(y_cv, y_predicted , labels= clf.classes_, eps=1e-15))

y_predicted = sig_clf.predict_proba(test_gene_onehotencode)
print("For value of best alpha", alpha[best_alpha], " the logloss is: ",log_loss(y_test, y_predicted, labels=clf.classes_, eps=1e-15))

"""Hence by observing Train, Cross Validate and Test logloss, it is clear that our Logistic Regression Model is not Overfitting and Underfitting."""

print("Finding how many datapoint of Test and Cross Validate are present in Train dataset.")
test_coverage = x_test[x_test['Gene'].isin(list(set(x_train['Gene'])))].shape[0]
cv_coverage   = x_cv[x_cv['Gene'].isin(list(set(x_train['Gene'])))].shape[0]
print("Out of ",x_test.shape[0] , " datapoints", test_coverage , " are present in Train dataset.", np.round((test_coverage/x_test.shape[0])*100 , 3),"%")
print("Out of ",x_cv.shape[0], " datapoints", cv_coverage , " are present in Train dataset.", np.round((cv_coverage/x_cv.shape[0])*100 , 3),"%")

"""Hence, it is clear that all the three (Train, CrossValidate and Test) datasets follows similar distribution.

<h3> Univarient Analysis of Variation Feature
"""

unique_var = x_train['Variation'].value_counts()
top_10_var = x_train.Variation.value_counts()
print("Top 10 Variation")
top_10_var.head(10)

# plotting histogram
s = sum(unique_var.values)
h = unique_var.values/s
plt.plot(h, label="Histogram for Variation Feature")
plt.xlabel('Variation')
plt.ylabel('Frequency')
plt.legend()
plt.grid()
plt.show()

# plotting Cumulative Distribution
c = np.cumsum(h)
plt.plot(c, label="Cumulative Distribution for Variation Feature", c='g')
plt.xlabel("Number of Variation")
plt.ylabel('Occurance')
plt.legend()
plt.grid()
plt.show()

"""<h3> Calling Response Coding for Variation feature"""

alpha =1
train_var_ResponseCode = np.array(get_gv_feature(alpha, 'Variation', x_train))
cv_var_ResponseCode     = np.array(get_gv_feature(alpha, 'Variation', x_cv))
test_var_ResponseCode   = np.array(get_gv_feature(alpha, 'Variation', x_test))
print("After Response Coding Train dataset looks like: ",train_var_ResponseCode.shape)
print("After Response Coding Cross Validate dataset looks like: ",cv_var_ResponseCode.shape)
print("After Response Coding Test dataset looks like: ",test_var_ResponseCode.shape)

"""<h3>Variation Feature using OneHot Encoding"""

var_encoding = CountVectorizer()
train_var_onehotencode = var_encoding.fit_transform(x_train['Variation'])
cv_var_onehotencode    = var_encoding.transform(x_cv['Variation'])
test_var_onehotencode  = var_encoding.transform(x_test['Variation'])
print("Train dataset after applying One Hot Encoding looks like: ",train_var_onehotencode.shape)
print("Cross Validate dataset after applying One Hot Encoding looks like: ",cv_var_onehotencode.shape)
print("Test dataset after applying One Hot Encoding looks like: ",test_var_onehotencode.shape)

var_encoding.get_feature_names()

"""<h3> Modeling Variation feature with Logistic Regression using Response Coding"""

alpha = [10 ** x for x in range(-5,2)]
cv_log_error = []
for i in alpha:
  clf = SGDClassifier(alpha= i, penalty='l1',loss='log', n_jobs= -1, random_state=42)
  clf.fit(train_var_ResponseCode, y_train)
  sig_clf = CalibratedClassifierCV(clf, method='sigmoid')
  sig_clf.fit(train_var_ResponseCode, y_train)
  y_predicted = sig_clf.predict_proba(cv_var_ResponseCode)
  cv_log_error.append(log_loss(y_cv, y_predicted, labels=clf.classes_, eps=1e-15))
  print("For alpha ", i," logloss is: ",log_loss(y_cv, y_predicted, labels=clf.classes_, eps=1e-15))

fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error, c='g')
for i, txt in enumerate(np.round(cv_log_error,3)):
  ax.annotate((alpha[i], np.round(txt,3)), (alpha[i], cv_log_error[i]))
plt.title("Cross Validate errors for each alpha")
plt.xlabel("Alpha's")
plt.ylabel("Logloss")
plt.grid()
plt.show()


best_alpha = np.argmin(cv_log_error)

clf = SGDClassifier(alpha = alpha[best_alpha], penalty='l1',loss='log', n_jobs= -1, random_state=42)
clf.fit(train_var_ResponseCode, y_train)
sig_clf = CalibratedClassifierCV(clf, method='sigmoid')
sig_clf.fit(train_var_ResponseCode, y_train)

y_predicted = sig_clf.predict_proba(test_var_ResponseCode)
print("For alpha ", alpha[best_alpha]," the logloss on Test dataset is: ",log_loss(y_test, y_predicted, labels=clf.classes_, eps=1e-15))

y_predicted = sig_clf.predict_proba(train_var_ResponseCode)
print("For alpha ", alpha[best_alpha]," the logloss on Train dataset is: ",log_loss(y_train, y_predicted, labels=clf.classes_, eps=1e-15))

y_predicted = sig_clf.predict_proba(cv_var_ResponseCode)
print("For alpha ", alpha[best_alpha]," the logloss on Cross Validate dataset is: ",log_loss(y_cv, y_predicted, labels=clf.classes_, eps=1e-15))

"""<h3> Modeling Variation feature with Logistic Regression using OneHot Encoding"""

alpha = [10 ** x for x in range(-5,2)]
cv_log_error = []
for i in alpha:
  clf = SGDClassifier(alpha= i, penalty='l1',loss='log', n_jobs= -1, random_state=42)
  clf.fit(train_var_onehotencode, y_train)
  sig_clf = CalibratedClassifierCV(clf, method='sigmoid')
  sig_clf.fit(train_var_onehotencode, y_train)
  y_predicted = sig_clf.predict_proba(cv_var_onehotencode)
  cv_log_error.append(log_loss(y_cv, y_predicted, labels=clf.classes_, eps=1e-15))
  print("For alpha ", i," logloss is: ",log_loss(y_cv, y_predicted, labels=clf.classes_, eps=1e-15))

fig, ax = plt.subplots()
ax.plot(alpha, cv_log_error, c='g')
for i, txt in enumerate(np.round(cv_log_error,3)):
  ax.annotate((alpha[i], np.round(txt,3)), (alpha[i], cv_log_error[i]))
plt.title("Cross Validate errors for each alpha")
plt.xlabel("Alpha's")
plt.ylabel("Logloss")
plt.grid()
plt.show()


best_alpha = np.argmin(cv_log_error)

clf = SGDClassifier(alpha = alpha[best_alpha], penalty='l1',loss='log', n_jobs= -1, random_state=42)
clf.fit(train_var_onehotencode, y_train)
sig_clf = CalibratedClassifierCV(clf, method='sigmoid')
sig_clf.fit(train_var_onehotencode, y_train)

y_predicted = sig_clf.predict_proba(test_var_onehotencode)
print("For alpha ", alpha[best_alpha]," the logloss on Test dataset is: ",log_loss(y_test, y_predicted, labels=clf.classes_, eps=1e-15))

y_predicted = sig_clf.predict_proba(train_var_onehotencode)
print("For alpha ", alpha[best_alpha]," the logloss on Train dataset is: ",log_loss(y_train, y_predicted, labels=clf.classes_, eps=1e-15))

y_predicted = sig_clf.predict_proba(cv_var_onehotencode)
print("For alpha ", alpha[best_alpha]," the logloss on Cross Validate dataset is: ",log_loss(y_cv, y_predicted, labels=clf.classes_, eps=1e-15))

# Logistic reg With L1 regularization
# For alpha  0.001  the logloss on Test dataset is:  1.7134085717233352
# For alpha  0.001  the logloss on Train dataset is:  1.687610047601688
# For alpha  0.001  the logloss on Cross Validate dataset is:  1.6984516054770864

# Logistic reg With L2 regularization
# For alpha  0.0001  the logloss on Test dataset is:  1.7029377326637025
# For alpha  0.0001  the logloss on Train dataset is:  0.6604824786838963
# For alpha  0.0001  the logloss on Cross Validate dataset is:  1.689454970455003

"""Hence by observing Train, Cross Validate and Test logloss, it is clear that our Logistic Regression Model for Variation feature is not Overfitting and Underfitting."""

print("Finding how many data points for Test and Cross Validate are present in Train dataset.")
test_coverage = x_test[x_test['Variation'].isin(list(set(x_train['Variation'])))].shape[0]
cv_coverage = x_cv[x_cv['Variation'].isin(list(set(x_train['Variation'])))].shape[0]
print("Out of ",x_test.shape[0] ,"(Test) datapoints",  test_coverage, " are present in Train dataset.", "(",np.round((test_coverage/x_test.shape[0])*100,3),"%)")
print("Out of ",x_cv.shape[0] ,"(Cross Validate) datapoints",  cv_coverage, " are present in Train dataset.", "(",np.round((cv_coverage/x_cv.shape[0])*100,3),"%)")

"""Hence, it is clear that all the three (Train, CrossValidate and Test) datasets shares little amount of same distribution.

<h3> Univerant analysis of Text feature
"""

def extract_dictonary_paddle(cls_text):
  dictionary = defaultdict(int)   #defaultdict does not raise keyvalue error
  for index, row in cls_text.iterrows():
    for word in row['TEXT'].split():
      dictionary[word] += 1
  return dictionary

def get_response_code(df):
  text_feature_response_coding = np.zeros((df.shape[0],9))
  for i in range(0,9):
    row_index = 0
    for index, row in df.iterrows():
      sum_prob = 0
      for words in row['TEXT'].split():
        sum_prob += math.log(((dict_list[i].get(words,0)+10) / (total_dict.get(words,0)+90)))
      text_feature_response_coding[row_index][i] = math.exp(sum_prob/len(row['TEXT'].split()))
      row_index += 1
    return text_feature_response_coding

# buid a countvector for word occur minimum 3 times.
text_vectorizer = CountVectorizer(min_df=3)
train_text_onehotencode = text_vectorizer.fit_transform(x_train['TEXT'])
cv_text_onehotencode    = text_vectorizer.transform(x_cv['TEXT'])
test_text_onehotencode  = text_vectorizer.transform(x_test['TEXT'])
# get feature names
train_text_feat_names   = text_vectorizer.get_feature_names()

train_text_feature_count= train_text_onehotencode.sum(axis=0).A1

text_feat_dict          = dict(zip(list(train_text_feat_names), train_text_feature_count))
print("Number of unique words in dictionary: ",len(train_text_feat_names))

dict_list = []
for i in range(1,10):
  cls_text = x_train[x_train["Class"] == i]
  dict_list.append(extract_dictonary_paddle(cls_text))   #

total_dict = extract_dictonary_paddle(x_train)

confuse_array = []
for i in train_text_feat_names:
  ratio = []
  max_val = -1
  for j in range(0,9):
    ratio.append((dict_list[j][i] + 10) / (total_dict[i] + 90))
  confuse_array.append(ratio)
confuse_array = np.array(confuse_array)

"""Calling Response Coding function for Text dataset"""

train_text_responsecode = get_response_code(x_train)
cv_text_responsecode    = get_response_code(x_cv)
test_text_responsecode  = get_response_code(x_test)

"""Normalize Response Coding"""

train_text_feat_responsecode = ((train_text_responsecode.T) / (train_text_responsecode.sum(axis=1))).T
cv_text_feat_responsecode    = ((cv_text_responsecode.T) / (cv_text_responsecode.sum(axis=1))).T
test_text_feat_responsecode  = ((test_text_responsecode.T) / (test_text_responsecode.sum(axis=1))).T

"""Normalize OneHot Encoding"""

train_text_onehotencode = normalize(train_text_onehotencode , axis=0)
cv_text_onehotencode    = normalize(cv_text_onehotencode , axis =0)
test_text_onehotencode  = normalize(test_text_onehotencode , axis =0)

alpha = [10 ** x for x in range(-5,2)]
cv_log_error = []
for i in alpha:
  clf = SGDClassifier(alpha = i , penalty = 'l1' , loss= 'log' , random_state = 42 , n_jobs=-1)
  clf.fit(train_text_onehotencode , y_train)
  sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
  sig_clf.fit(train_text_onehotencode , y_train)
  y_predicted = sig_clf.predict_proba(cv_text_onehotencode)
  cv_log_error.append(log_loss(y_cv , y_predicted , labels = clf.classes_ , eps=1e-15))
  print("For alpha ",i," the log loss is: ", log_loss(y_cv , y_predicted , labels= clf.classes_ , eps= 1e-15))

fig , ax = plt.subplots()
ax.plot(alpha , cv_log_error , c='g')
for i , txt in enumerate(np.round(cv_log_error , 3)):
  ax.annotate((alpha[i] , np.round(txt,3)) , (alpha[i] , cv_log_error[i]))
plt.title("Cross Validate error on each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Logloss")
plt.grid()
plt.show()

best_alpha = np.argmin(cv_log_error)

clf = SGDClassifier(alpha = alpha[best_alpha] , penalty = 'l1', loss = 'log', random_state=42, n_jobs=-1)
clf.fit(train_text_onehotencode , y_train)
sig_clf = CalibratedClassifierCV(clf , method = 'sigmoid')
sig_clf.fit(train_text_onehotencode , y_train)
y_predicted = sig_clf.predict_proba(test_text_onehotencode)
print("For alpha ",alpha[best_alpha],"the test logloss is: ", log_loss(y_test , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(cv_text_onehotencode)
print("For alpha ",alpha[best_alpha],"the cross validate logloss is: ", log_loss(y_cv , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(train_text_onehotencode)
print("For alpha ",alpha[best_alpha],"the train logloss is: ", log_loss(y_train , y_predicted , labels= clf.classes_ , eps=1e-15))

def get_intersect_text(df_text):
  text_vectorizer = CountVectorizer()
  text_feat       = text_vectorizer.fit_transform(df_text['TEXT'])
  text_feat_names = text_vectorizer.get_feature_names()
  text_feat_count = text_feat.sum(axis=0).A1
  text_feat_dict  = dict(zip(list(text_feat_names),text_feat_count))

  len1 = len(set(text_feat_names))
  len2 = len(set(train_text_feat_names) & set(text_feat_names))
  return len1 , len2

len1 , len2 = get_intersect_text(x_cv)
print(np.round((len2/len1) *100,3), "%  of cross_validate dataset are present in train dataset.")
len1 , len2 = get_intersect_text(x_test)
print(np.round((len2/len1) *100 , 3), "%  of test dataset are present in train dataset.")

"""<h3> Heap Stack Train, Cross Validate and Test dataset (OneHotEncoding)."""

train_gene_var_onehotencode = hstack((train_gene_onehotencode , train_var_onehotencode))
cv_gene_var_onehotencode    = hstack((cv_gene_onehotencode , cv_var_onehotencode))
test_gene_var_onehotencode  = hstack((test_gene_onehotencode , test_var_onehotencode))

train_onehotencode = hstack((train_gene_var_onehotencode , train_text_onehotencode)).tocsr()
cv_onehotencode    = hstack((cv_gene_var_onehotencode , cv_text_onehotencode)).tocsr()
test_onehotencode  = hstack((test_gene_var_onehotencode , test_text_onehotencode)).tocsr()

y_train_onehotencode = np.array(list(x_train['Class']))
y_cv_onehotencode    = np.array(list(x_cv['Class']))
y_test_onehotencode  = np.array(list(x_test['Class']))

print("Train Y onehotencode",y_train_onehotencode.shape)
print("Train X onehotencode",train_onehotencode.shape)

"""<h3>Stacking after OneHot Encoding"""

print("Train dataset after stacking:",train_onehotencode.shape)
print("Cross Validate dataset after stacking:",cv_onehotencode.shape)
print("Test dataset after stacking:",test_onehotencode.shape)

"""<h3> Heap Stack Train, Cross Validate and Test dataset (Response Coding)."""

train_gene_var_ResponseCode = hstack((csr_matrix(train_gene_ResponseCoding) , train_var_ResponseCode))
cv_gene_var_ResponseCode    = hstack((csr_matrix(cv_gene_ResponseCoding) , cv_var_ResponseCode))
test_gene_var_ResponseCode  = hstack((csr_matrix(test_gene_ResponseCoding) , test_var_ResponseCode))
# without csr_matrix, atleast one dataset must have sparse array.
# ValueError: blocks must be 2-D

train_ResponseCode = hstack((train_gene_var_ResponseCode , train_text_feat_responsecode))
cv_ResponseCode    = hstack((cv_gene_var_ResponseCode , cv_text_feat_responsecode))
test_ResponseCode  = hstack((test_gene_var_ResponseCode , test_text_feat_responsecode))

"""<h3>Stacking after Response Coding"""

print("Train dataset after stacking:",train_ResponseCode.shape)
print("Cross Validate dataset after stacking:",cv_ResponseCode.shape)
print("Test dataset after stacking:",test_ResponseCode.shape)

"""<h3> Function that is use for feature importance for Naive Bayes."""

def get_impfeature_names(indices, text, gene, var, no_features):
    gene_count_vec = CountVectorizer()
    var_count_vec = CountVectorizer()
    text_count_vec = CountVectorizer(min_df=3)

    gene_vec = gene_count_vec.fit(x_train['Gene'])
    var_vec  = var_count_vec.fit(x_train['Variation'])
    text_vec = text_count_vec.fit(x_train['TEXT'])


    fea1_len = len(gene_vec.get_feature_names())
    fea2_len = len(var_count_vec.get_feature_names())

    word_present = 0
    for i,v in enumerate(indices):
        if (v < fea1_len):
            word = gene_vec.get_feature_names()[v]
            yes_no = True if word == gene else False
            if yes_no:
                word_present += 1
                print(i, "Gene feature [{}] present in test data point [{}]".format(word,yes_no))
        elif (v < fea1_len+fea2_len):
            word = var_vec.get_feature_names()[v-(fea1_len)]
            yes_no = True if word == var else False
            if yes_no:
                word_present += 1
                print(i, "variation feature [{}] present in test data point [{}]".format(word,yes_no))
        else:
            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]
            yes_no = True if word in text.split() else False
            if yes_no:
                word_present += 1
                print(i, "Text feature [{}] present in test data point [{}]".format(word,yes_no))

    print("Out of the top ",no_features," features ", word_present, "are present in query point")

"""<h3>Function to predict and plot confusion matrix."""

def predict_and_plot_confusion_matrix(x_train, y_train, x_test, y_test, clf):
  clf.fit(x_train , y_train)
  sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
  sig_clf.fit(x_train , y_train)
  y_predicted_class = sig_clf.predict(x_test)
  y_predicted = sig_clf.predict_proba(x_test)

  print("Logloss is: ", log_loss(y_test , y_predicted , labels=clf.classes_, eps=1e-15))
  #print("Number of misclassified:", np.count_nonzero((y_predicted-y_test))/y_test.shape[0])
  print("Number of mis-classified points :", np.count_nonzero((y_predicted_class- y_test))/y_test.shape[0])
  plot_confusion_matrix(y_test , y_predicted_class)

"""<h3> Function to report logloss."""

def report_logloss(x_train, y_train, x_test, y_test, clf):
  clf.fit(x_train , y_train)
  sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
  sig_clf.fit(x_train , y_train)
  y_predicted = sig_clf.predict_proba(x_test)
  return log_loss(y_test, y_predicted, label=clf.classes_ , eps=1e-15)

alpha = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]
cv_log_error = []

for i in alpha:
  clf = MultinomialNB(alpha = i)
  clf.fit(train_onehotencode , y_train_onehotencode)
  sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
  sig_clf.fit(train_onehotencode, y_train_onehotencode)
  y_predicted = sig_clf.predict_proba(cv_onehotencode)
  print("logloss for alpha",i,"is:",log_loss(y_cv_onehotencode , y_predicted, labels= clf.classes_, eps=1e-15))
  cv_log_error.append(log_loss(y_cv_onehotencode , y_predicted, labels= clf.classes_, eps=1e-15))

fig, ax = plt.subplots()
ax.plot(np.log10(alpha), cv_log_error, c='g')
for i, txt in enumerate(np.round(cv_log_error, 3)):
  ax.annotate((alpha[i],str(txt)) , (np.log10(alpha[i]), np.round(cv_log_error[i],3)))
plt.grid()
plt.xticks(np.log10(alpha))
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.title("Cross Validate alphas logloss")
plt.show()

best_alpha = np.argmin(cv_log_error)
clf = MultinomialNB(alpha= alpha[best_alpha])
clf.fit(train_onehotencode , y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf, method = 'sigmoid')
sig_clf.fit(train_onehotencode, y_train_onehotencode)

y_predicted = sig_clf.predict_proba(cv_onehotencode)
print("Logloss for best alpha",alpha[best_alpha], "on cross validate data is:",log_loss(y_cv_onehotencode, y_predicted, labels=clf.classes_, eps=1e-15))

y_predicted = sig_clf.predict_proba(train_onehotencode)
print("Logloss for best alpha", alpha[best_alpha], "on train data is:", log_loss(y_train_onehotencode, y_predicted, labels= clf.classes_, eps=1e-15))

y_predicted = sig_clf.predict_proba(test_onehotencode)
print("Logloss for best alpha",alpha[best_alpha], "on test data is:", log_loss(y_test_onehotencode, y_predicted, labels=clf.classes_, eps=1e-15))

clf = MultinomialNB(alpha = alpha[best_alpha])
clf.fit(train_onehotencode, y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
sig_clf.fit(train_onehotencode, y_train_onehotencode)
y_predicted = sig_clf.predict_proba(cv_onehotencode)    # use sig_clf.predict_proba
print("Logloss:", log_loss(y_cv_onehotencode , y_predicted))
print("Number of misclassified points: ",np.count_nonzero((sig_clf.predict(cv_onehotencode) - y_cv_onehotencode)) / y_cv_onehotencode.shape[0])     # use sig_clf.predict
plot_confusion_matrix(y_cv_onehotencode, sig_clf.predict(cv_onehotencode.toarray()))

"""<h3> Testing the MultinomialNB model."""

clf = MultinomialNB(alpha= alpha[best_alpha])
clf.fit(train_onehotencode,y_train_onehotencode)
test_point_index = 1
no_features = 100
predicted_cls = sig_clf.predict(test_onehotencode[test_point_index])
print("Predicted Class :", predicted_cls[0])
print("Predicted Class Probabilities:", np.round(sig_clf.predict_proba(test_onehotencode[test_point_index]),4))
print("Actual Class :", y_test_onehotencode[test_point_index])
indices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_features]
print("-"*50)
get_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_features)

"""<h3> Modelling with KNN using Response Coding."""

cv_log_error = []
alpha = [3, 5, 7, 9, 13, 17, 33, 53, 77, 99, 103, 109]
for i in alpha:
  clf = KNeighborsClassifier(n_jobs=-1, n_neighbors= i)
  clf.fit(train_ResponseCode, y_train_onehotencode)
  sig_clf = CalibratedClassifierCV(clf , method="sigmoid")
  sig_clf.fit(train_ResponseCode, y_train_onehotencode)
  predicted_y = sig_clf.predict_proba(cv_ResponseCode)
  cv_log_error.append(log_loss(y_cv_onehotencode, predicted_y, labels = clf.classes_ , eps=1e-15))
  print("Logloss is: ", log_loss(y_cv_onehotencode, predicted_y, labels = clf.classes_ , eps=1e-15))

fig,ax = plt.subplots()
ax.plot(alpha , cv_log_error , c='g')
for i , txt in enumerate(np.round(cv_log_error,3)):
  ax.annotate((alpha[i], str(txt)) , (alpha[i] , np.round(cv_log_error[i],3)))
plt.xlabel("Alpha i's")
plt.ylabel("Error loss")
plt.title("Cross Validate Log loss")
plt.grid()
plt.show()

best_alpha = np.argmin(cv_log_error)
clf = KNeighborsClassifier(n_jobs= -1, n_neighbors= alpha[best_alpha])
clf.fit(train_ResponseCode, y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf, method='sigmoid')
sig_clf.fit(train_ResponseCode, y_train_onehotencode)

predicted_y = sig_clf.predict_proba(test_ResponseCode)
print("Logloss for best alpha" , alpha[best_alpha],"Test data: ",log_loss(y_test_onehotencode , predicted_y , labels = clf.classes_, eps=1e-15))

predicted_y = sig_clf.predict_proba(train_ResponseCode)
print("Logloss for " , alpha[best_alpha],"Train data: ",log_loss(y_train_onehotencode , predicted_y , labels = clf.classes_, eps=1e-15))

predicted_y = sig_clf.predict_proba(cv_ResponseCode)
print("Logloss for " , alpha[best_alpha]," Cross Validate data: ",log_loss(y_cv_onehotencode , predicted_y , labels = clf.classes_, eps=1e-15))

clf = KNeighborsClassifier(n_neighbors = alpha[best_alpha] , n_jobs= -1)
predict_and_plot_confusion_matrix(train_ResponseCode, y_train_onehotencode , cv_ResponseCode, y_cv_onehotencode , clf)

"""<h3> Modelling with KNN using OneHot Encoding."""

cv_log_error = []
alpha = [3, 5, 7, 9, 13, 17, 33, 53, 77, 99, 103, 109]
for i in alpha:
  clf = KNeighborsClassifier(n_jobs=-1, n_neighbors= i)
  clf.fit(train_onehotencode, y_train_onehotencode)
  sig_clf = CalibratedClassifierCV(clf , method="sigmoid")
  sig_clf.fit(train_onehotencode, y_train_onehotencode)
  predicted_y = sig_clf.predict_proba(cv_onehotencode)
  cv_log_error.append(log_loss(y_cv, predicted_y, labels = clf.classes_ , eps=1e-15))
  print("Logloss is: ", log_loss(y_cv, predicted_y, labels = clf.classes_ , eps=1e-15))

fig,ax = plt.subplots()
ax.plot(alpha , cv_log_error , c='g')
for i , txt in enumerate(np.round(cv_log_error,3)):
  ax.annotate((alpha[i], str(txt)) , (alpha[i] , np.round(cv_log_error[i],3)))
plt.xlabel("Alpha i's")
plt.ylabel("Error loss")
plt.title("Cross Validate Log loss")
plt.grid()
plt.show()

best_alpha = np.argmin(cv_log_error)
clf = KNeighborsClassifier(n_jobs= -1, n_neighbors= alpha[best_alpha])
clf.fit(train_onehotencode, y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf, method='sigmoid')
sig_clf.fit(train_onehotencode, y_train_onehotencode)

predicted_y = sig_clf.predict_proba(test_onehotencode)
print("Logloss for best alpha" , alpha[best_alpha],"Test data: ",log_loss(y_test_onehotencode , predicted_y , labels = clf.classes_, eps=1e-15))

predicted_y = sig_clf.predict_proba(train_onehotencode)
print("Logloss for " , alpha[best_alpha],"Train data: ",log_loss(y_train_onehotencode , predicted_y , labels = clf.classes_, eps=1e-15))

predicted_y = sig_clf.predict_proba(cv_onehotencode)
print("Logloss for " , alpha[best_alpha]," Cross Validate data: ",log_loss(y_cv_onehotencode , predicted_y , labels = clf.classes_, eps=1e-15))

test_datapoint = 1
clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=alpha[best_alpha])
clf.fit(train_ResponseCode, y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf, method= 'sigmoid')
predicted_class = sig_clf.predict(test_ResponseCode[test_datapoint])

"""<h3>Modelling by Logistic Regression with balance class weights using OneHot Encoding."""

alpha = [10 ** x for x in range(-5,2)]
cv_log_error = []
for i in alpha:
  clf = SGDClassifier(alpha = i , class_weight = 'balanced', penalty = 'l2' , loss= 'log' , random_state = 42 , n_jobs=-1)
  clf.fit(train_onehotencode , y_train_onehotencode)
  sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
  sig_clf.fit(train_onehotencode , y_train_onehotencode)

  y_predicted = sig_clf.predict_proba(cv_onehotencode)
  cv_log_error.append(log_loss(y_cv_onehotencode , y_predicted , labels = clf.classes_ , eps=1e-15))
  print("For alpha ",i," the log loss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps= 1e-15))

fig , ax = plt.subplots()
ax.plot(np.log10(alpha) , cv_log_error , c='g')
for i , txt in enumerate(np.round(cv_log_error , 3)):
  ax.annotate((alpha[i] , np.round(txt,3)) , (np.log10(alpha[i]) , cv_log_error[i]))
plt.title("Cross Validate error on each alpha")
plt.xticks(np.log10(alpha))
plt.xlabel("Alpha i's")
plt.ylabel("Logloss")
plt.grid()
plt.show()

best_alpha = np.argmin(cv_log_error)

clf = SGDClassifier(alpha = alpha[best_alpha] ,class_weight='balanced' , penalty = 'l2', loss = 'log', random_state=42, n_jobs=-1)
clf.fit(train_text_onehotencode , y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf , method = 'sigmoid')
sig_clf.fit(train_onehotencode , y_train_onehotencode)
y_predicted = sig_clf.predict_proba(test_onehotencode)
print("For alpha ",alpha[best_alpha],"the test logloss is: ", log_loss(y_test_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(cv_onehotencode)
print("For alpha ",alpha[best_alpha],"the cross validate logloss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(train_onehotencode)
print("For alpha ",alpha[best_alpha],"the train logloss is: ", log_loss(y_train_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))

clf = SGDClassifier(alpha = alpha[best_alpha] , class_weight = 'balanced', penalty = 'l2' , loss= 'log' , random_state = 42 , n_jobs=-1)
predict_and_plot_confusion_matrix(train_onehotencode , y_train_onehotencode, cv_onehotencode, y_cv_onehotencode, clf)

"""<h3>Modelling by Logistic Regression without balance class weights using OneHot Encoding."""

alpha = [10 ** x for x in range(-5,2)]
cv_log_error = []
for i in alpha:
  clf = SGDClassifier(alpha = i , penalty = 'l2' , loss= 'log' , random_state = 42 , n_jobs=-1)
  clf.fit(train_onehotencode , y_train_onehotencode)
  sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
  sig_clf.fit(train_onehotencode , y_train_onehotencode)
  y_predicted = sig_clf.predict_proba(cv_onehotencode)
  cv_log_error.append(log_loss(y_cv_onehotencode , y_predicted , labels = clf.classes_ , eps=1e-15))
  print("For alpha ",i," the log loss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps= 1e-15))

fig , ax = plt.subplots()
ax.plot(np.log10(alpha) , cv_log_error , c='g')
for i , txt in enumerate(np.round(cv_log_error , 3)):
  ax.annotate((alpha[i] , np.round(txt,3)) , (np.log10(alpha[i]) , cv_log_error[i]))
plt.title("Cross Validate error on each alpha")
plt.xticks(np.log10(alpha))
plt.xlabel("Alpha i's")
plt.ylabel("Logloss")
plt.grid()
plt.show()

best_alpha = np.argmin(cv_log_error)

clf = SGDClassifier(alpha = alpha[best_alpha] , penalty = 'l2', loss = 'log', random_state=42, n_jobs=-1)
clf.fit(train_text_onehotencode , y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf , method = 'sigmoid')
sig_clf.fit(train_onehotencode , y_train_onehotencode)
y_predicted = sig_clf.predict_proba(test_onehotencode)
print("For alpha ",alpha[best_alpha],"the test logloss is: ", log_loss(y_test_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(cv_onehotencode)
print("For alpha ",alpha[best_alpha],"the cross validate logloss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(train_onehotencode)
print("For alpha ",alpha[best_alpha],"the train logloss is: ", log_loss(y_train_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))

clf = SGDClassifier(alpha = alpha[best_alpha] , penalty = 'l2' , loss= 'log' , random_state = 42 , n_jobs=-1)
predict_and_plot_confusion_matrix(train_onehotencode , y_train_onehotencode, cv_onehotencode, y_cv_onehotencode, clf)

clf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)
clf.fit(train_onehotencode,y_train_onehotencode)
test_point_index = 1
no_features = 100
predicted_cls = sig_clf.predict(test_onehotencode[test_point_index])
print("Predicted Class :", predicted_cls[0], '\n',np.max(np.round(sig_clf.predict_proba(test_onehotencode[test_point_index]),4)*100), "% ")
print("Predicted Class Probabilities:", np.round(sig_clf.predict_proba(test_onehotencode[test_point_index]),4))
print("Actual Class :", y_test_onehotencode[test_point_index] )
indices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_features]
print("-"*50)
get_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_features)

alpha = [10 ** x for x in range(-5,2)]
cv_log_error = []
for i in alpha:
  clf = SGDClassifier(alpha = i , class_weight = 'balanced', penalty = 'l2' , loss= 'hinge' , random_state = 42 , n_jobs=-1)
  clf.fit(train_onehotencode , y_train_onehotencode)
  sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
  sig_clf.fit(train_onehotencode , y_train_onehotencode)

  y_predicted = sig_clf.predict_proba(cv_onehotencode)
  cv_log_error.append(log_loss(y_cv_onehotencode , y_predicted , labels = clf.classes_ , eps=1e-15))
  print("For C= ",i," the log loss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps= 1e-15))

fig , ax = plt.subplots()
ax.plot(np.log10(alpha) , cv_log_error , c='g')
for i , txt in enumerate(np.round(cv_log_error , 3)):
  ax.annotate((alpha[i] , np.round(txt,3)) , (np.log10(alpha[i]) , cv_log_error[i]))
plt.title("Cross Validate error on each alpha")
plt.xticks(np.log10(alpha))
plt.xlabel("Alpha i's")
plt.ylabel("Logloss")
plt.grid()
plt.show()

best_alpha = np.argmin(cv_log_error)

clf = SGDClassifier(alpha = alpha[best_alpha] ,class_weight='balanced' , penalty = 'l2', loss = 'hinge', random_state=42, n_jobs=-1)
clf.fit(train_text_onehotencode , y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf , method = 'sigmoid')
sig_clf.fit(train_onehotencode , y_train_onehotencode)
y_predicted = sig_clf.predict_proba(test_onehotencode)
print("For C= ",alpha[best_alpha],"the test logloss is: ", log_loss(y_test_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(cv_onehotencode)
print("For C= ",alpha[best_alpha],"the cross validate logloss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(train_onehotencode)
print("For C= ",alpha[best_alpha],"the train logloss is: ", log_loss(y_train_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))

clf = SGDClassifier(alpha = alpha[best_alpha] ,class_weight='balanced' , penalty = 'l2', loss = 'hinge', random_state=42, n_jobs=-1)
predict_and_plot_confusion_matrix(train_onehotencode, y_train_onehotencode , cv_onehotencode, y_cv_onehotencode, clf)

clf = SGDClassifier(alpha=alpha[best_alpha], class_weight='balanced', penalty='l2', loss='hinge', random_state=42)
clf.fit(train_onehotencode,y_train_onehotencode)
test_point_index = 1
no_features = 500
predicted_cls = sig_clf.predict(test_onehotencode[test_point_index])
print("Predicted Class :", predicted_cls[0], '\n',np.max(np.round(sig_clf.predict_proba(test_onehotencode[test_point_index]),4)*100), "% ")
print("Predicted Class Probabilities:", np.round(sig_clf.predict_proba(test_onehotencode[test_point_index]),4))
print("Actual Class :", y_test_onehotencode[test_point_index] )
indices = np.argsort(-1*abs(clf.coef_))[predicted_cls-1][:,:no_features]
print("-"*50)
get_impfeature_names(indices[0], x_test['TEXT'].iloc[test_point_index],x_test['Gene'].iloc[test_point_index],x_test['Variation'].iloc[test_point_index], no_features)

""" <h3>Modelling with Random Forest using OneHot Encoding.

"""

alpha = [5,10,155,125,225,1000]
max_depth = [5,10]
cv_log_error = []
for i in alpha:
  for j in max_depth:
    clf = RandomForestClassifier(n_estimators = i, max_depth=j, criterion='gini' , random_state = 42 , n_jobs=-1)
    clf.fit(train_onehotencode , y_train_onehotencode)
    sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
    sig_clf.fit(train_onehotencode , y_train_onehotencode)

    y_predicted = sig_clf.predict_proba(cv_onehotencode)
    cv_log_error.append(log_loss(y_cv_onehotencode , y_predicted , labels = clf.classes_ , eps=1e-15))
    print("For n_estimators= ",i," and the max_depth ",j, "logloss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps= 1e-15))

fig , ax = plt.subplots()
features = np.dot(np.array(alpha)[:,None] , np.array(max_depth)[None]).ravel()
ax.plot((features) , cv_log_error , c='g')
for i , txt in enumerate(np.round(cv_log_error , 3)):
  ax.annotate((alpha[int(i/2)] , max_depth[int(j%2)] , str(txt)) ,(features[i] , cv_log_error[i]))
plt.title("Cross Validate error on each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.grid()
plt.show()

best_alpha = np.argmin(cv_log_error)

clf = RandomForestClassifier(n_estimators = alpha[int(best_alpha/2)] ,max_depth=max_depth[int(j%2)], criterion = 'gini', random_state=42, n_jobs=-1)
clf.fit(train_onehotencode , y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf , method = 'sigmoid')
sig_clf.fit(train_onehotencode , y_train_onehotencode)
y_predicted = sig_clf.predict_proba(test_onehotencode)
print("For value of best estimator ",alpha[int(best_alpha/2)],"the test logloss is: ", log_loss(y_test_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(cv_onehotencode)
print("For value of best estimator ",alpha[int(best_alpha/2)],"the cross validate logloss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(train_onehotencode)
print("For value of best estimator ",alpha[int(best_alpha/2)],"the train logloss is: ", log_loss(y_train_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))

clf = RandomForestClassifier(n_estimators = alpha[int(best_alpha/2)] ,max_depth=max_depth[int(j%2)], criterion = 'gini', random_state=42, n_jobs=-1)
predict_and_plot_confusion_matrix(train_onehotencode, y_train_onehotencode, cv_onehotencode, y_cv_onehotencode, clf)

""" <h3>Modelling with Random Forest using Response Coding."""

alpha = [5,10,15,25,500,1000]
max_depth = [5,10]
cv_log_error = []
for i in alpha:
  for j in max_depth:
    clf = RandomForestClassifier(n_estimators = i, max_depth=j, criterion='gini' , random_state = 42 , n_jobs=-1)
    clf.fit(train_ResponseCode , y_train_onehotencode)
    sig_clf = CalibratedClassifierCV(clf , method='sigmoid')
    sig_clf.fit(train_ResponseCode , y_train_onehotencode)

    y_predicted = sig_clf.predict_proba(cv_ResponseCode)
    cv_log_error.append(log_loss(y_cv_onehotencode , y_predicted , labels = clf.classes_ , eps=1e-15))
    print("For n_estimators= ",i," and the max_depth ",j, "logloss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps= 1e-15))

fig , ax = plt.subplots()
features = np.dot(np.array(alpha)[:,None] , np.array(max_depth)[None]).ravel()
ax.plot((features) , cv_log_error , c='g')
for i , txt in enumerate(np.round(cv_log_error , 3)):
  ax.annotate((alpha[int(i/2)] , max_depth[int(j%2)] , str(txt)) , (features[i] , cv_log_error[i]))
plt.title("Cross Validate error on each alpha")
plt.xlabel("Alpha i's")
plt.ylabel("Error measure")
plt.grid()
plt.show()

best_alpha = np.argmin(cv_log_error)

clf = RandomForestClassifier(n_estimators = alpha[int(best_alpha/2)] ,max_depth=max_depth[int(j%2)], criterion = 'gini', random_state=42, n_jobs=-1)
clf.fit(train_ResponseCode , y_train_onehotencode)
sig_clf = CalibratedClassifierCV(clf , method = 'sigmoid')
sig_clf.fit(train_ResponseCode , y_train_onehotencode)
y_predicted = sig_clf.predict_proba(test_ResponseCode)
print("For value of best estimator ",alpha[int(best_alpha/2)],"the test logloss is: ", log_loss(y_test_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(cv_ResponseCode)
print("For value of best estimator ",alpha[int(best_alpha/2)],"the cross validate logloss is: ", log_loss(y_cv_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))
y_predicted = sig_clf.predict_proba(train_ResponseCode)
print("For value of best estimator ",alpha[int(best_alpha/2)],"the train logloss is: ", log_loss(y_train_onehotencode , y_predicted , labels= clf.classes_ , eps=1e-15))

clf = RandomForestClassifier(n_estimators = alpha[int(best_alpha/2)] ,max_depth=max_depth[int(j%2)], criterion = 'gini', random_state=42, n_jobs=-1)
predict_and_plot_confusion_matrix(train_ResponseCode, y_train_onehotencode, cv_ResponseCode, y_cv_onehotencode, clf)
